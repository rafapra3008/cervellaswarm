# RICERCA HARDWARE AI FISICO - NVIDIA GPU per Self-Hosting LLM 2026

**Data ricerca**: 11 Gennaio 2026
**Ricercatrice**: Cervella Researcher
**Modello target**: Qwen3-4B Q4_K_M (4-6GB VRAM) + scalabilit√† futura
**Location**: Albergo con infrastruttura esistente (PC Windows, switch, firewall)

---

## EXECUTIVE SUMMARY

### TL;DR
- **VRAM minima Qwen3-4B Q4_K_M**: ~2-4GB (quantizzato 4-bit), ~7GB (FP16)
- **Opzione BEST VALUE**: RTX 3090 usata 24GB (~‚Ç¨800-900) - 24GB permettono scaling futuro
- **Break-even vs cloud**: >2M token/giorno O requisiti compliance (HIPAA/PCI)
- **Consumo elettrico RTX 3090 24/7**: ~‚Ç¨650-1400/anno (dipende da ‚Ç¨/kWh)
- **Raccomandazione**: Iniziare con RTX 3090 usata, valutare cloud per picchi

---

## 1. NVIDIA JETSON - Edge AI Platform

### Modelli & Prezzi

| Modello | VRAM | Prezzo | Performance | Ideale Per |
|---------|------|--------|-------------|------------|
| **Jetson Orin Nano Super** | 8GB | $249 | 67 TOPS | Proof of concept, edge |
| **Jetson AGX Orin 32GB** | 32GB | ~$999 | 275 TOPS | Modelli 4B-13B |
| **Jetson AGX Orin 64GB** | 64GB | **$1,999** | 275 TOPS | Modelli 4B-20B, VLM |

### Capacit√† LLM Inference

- **Qwen3-4B**: ‚úÖ Ottimale su Orin Nano Super ($249) o AGX Orin 32GB
- **Modelli 7B-13B**: AGX Orin 64GB con TensorRT-LLM
- **VLM (Vision)**: LLaVA-13B, Qwen2.5-VL-7B, Phi-3.5-Vision su AGX Orin 64GB

### Pro & Contro

**PRO:**
- Consumo energetico molto basso (15-60W configurabile)
- Forma fattore compatta, silenzioso
- Ottimizzato per edge inference (TensorRT-LLM)
- Prezzo accessibile per entry-level

**CONTRO:**
- Performance inferiore a GPU desktop (RTX serie)
- Memoria unificata limita batch size
- Non espandibile (memoria saldata)
- Ecosistema pi√π limitato rispetto a CUDA standard

### Quando Scegliere Jetson
‚úÖ Deployment edge con vincoli energetici stretti
‚úÖ Proof of concept prima di investimento grosso
‚úÖ Installazioni remote con budget energia limitato
‚ùå Se serve scalare oltre 20B modelli
‚ùå Se serve high-throughput batch inference

**Fonti:**
- [NVIDIA Jetson AGX Orin Official](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)
- [Buy Jetson Products](https://developer.nvidia.com/buy-jetson)
- [Amazon - Jetson AGX Orin 64GB Developer Kit](https://www.amazon.com/NVIDIA-Jetson-Orin-64GB-Developer/dp/B0BYGB3WV4)
- [Getting Started with Edge AI on Jetson](https://developer.nvidia.com/blog/getting-started-with-edge-ai-on-nvidia-jetson-llms-vlms-and-foundation-models-for-robotics/)

---

## 2. NVIDIA RTX WORKSTATION - Consumer & Professional GPU

### RTX 4090 vs RTX 6000 Ada - Confronto Dettagliato

| Specifica | **RTX 4090** | **RTX 6000 Ada** |
|-----------|--------------|------------------|
| **VRAM** | 24GB GDDR6X | **48GB ECC GDDR6** |
| **Memory Bandwidth** | 1008 GB/s | 960 GB/s |
| **FP32 Performance** | ~82 TFLOPS | 91.1 TFLOPS |
| **Tensor Cores** | 4th Gen | 4th Gen |
| **TDP** | 450W | 300W |
| **Cooling** | Consumer | Workstation (blower) |
| **ECC Memory** | ‚ùå | ‚úÖ |
| **vGPU Support** | ‚ùå | ‚úÖ |
| **Prezzo Nuovo** | ~‚Ç¨1,800-2,000 | ~‚Ç¨6,800-7,200 |
| **Prezzo Usato** | ~‚Ç¨1,600-1,900 | N/A (poco mercato) |
| **Value for Money** | üèÜ **8.5x migliore** | Baseline |

### Performance LLM Inference

**RTX 4090:**
- Modelli 7B-30B quantizzati: üöÄ ~112 token/s (8B modelli)
- Batch size: Limitato a 24GB (sufficiente per servire 1-5 utenti)
- Use case: Personal chatbot, sviluppo, piccoli team

**RTX 6000 Ada:**
- Modelli fino a 70B quantizzati: üöÄ ~3.7x pi√π veloce del 4090 (workload specifici)
- Batch size: Doppio rispetto a 4090 (serve pi√π utenti concorrenti)
- Use case: Production 24/7, multi-user, enterprise reliability

### Quando Scegliere Cosa

**Scegli RTX 4090 se:**
- ‚úÖ Budget limitato (~‚Ç¨2k nuova, ~‚Ç¨1.6k usata)
- ‚úÖ Modelli 7B-30B quantizzati
- ‚úÖ Uso personale/team piccolo (1-5 utenti)
- ‚úÖ Non serve uptime 24/7 mission-critical

**Scegli RTX 6000 Ada se:**
- ‚úÖ Serve 24/7 uptime (ECC memory)
- ‚úÖ Multi-user production (serve pi√π batch concorrenti)
- ‚úÖ Modelli >30B non quantizzati
- ‚úÖ Budget disponibile ‚Ç¨7k+
- ‚úÖ vGPU/virtualizzazione necessaria

### RTX 3090 - BEST VALUE OPTION üèÜ

| Specifica | **RTX 3090** | Note |
|-----------|--------------|------|
| **VRAM** | **24GB GDDR6X** | Pari a 4090 |
| **Performance LLM** | ~70-80% del 4090 | 8B: ~80 token/s |
| **Prezzo Usato** | **‚Ç¨800-1,000** | üèÜ Miglior $/VRAM |
| **Prezzo Nuovo** | ~‚Ç¨1,100-1,300 | Ancora disponibile |
| **TDP** | 350W | -100W vs 4090 |
| **Mercato Usato** | üü¢ Molto disponibile | Mining boom passato |

**PERCH√â RTX 3090 √à LA SCELTA MIGLIORE:**
- 24GB VRAM = modelli fino a 30B quantizzati
- ‚Ç¨800 usata = 1/3 del prezzo RTX 6000 Ada per stessa VRAM
- Performance sufficiente per self-hosting (80 token/s su 8B)
- Disponibilit√† alta nel mercato usato
- Scaling futuro possibile (2x 3090 in dual-GPU = 48GB totale)

### Altri Modelli RTX da Considerare

| GPU | VRAM | Prezzo Usato | Use Case |
|-----|------|--------------|----------|
| **RTX 4080** | 16GB | ‚Ç¨900-1,100 | ‚ö†Ô∏è Meno VRAM ma pi√π veloce |
| **RTX 3090 Ti** | 24GB | ‚Ç¨1,000-1,300 | Leggermente meglio di 3090 |
| **RTX 4070 Ti** | 12GB | ‚Ç¨600-700 | ‚ùå Troppo poco VRAM |
| **RTX A5000** | 24GB | ‚Ç¨1,500-2,000 | Workstation, ECC, blower |

**Fonti:**
- [RTX 6000 Ada vs 4090 for AI](https://www.bestgpusforai.com/gpu-comparison/6000-ada-vs-4090)
- [RTX 4090 vs 6000 Ada for Self-Hosted vLLM](https://www.jolomo.io/rtx-4090-vs-rtx-6000-ada-for-self-hosted-vllm-interfaces/)
- [Best NVIDIA GPUs for LLM Inference 2025](https://www.databasemart.com/blog/best-nvidia-gpus-for-llm-inference-2025)
- [RTX 3090 vs 4080 Comparison](https://www.bestgpusfoai.com/gpu-comparison/3090-vs-4080)
- [RTX 3090 Pricing & Specs 2026](https://www.fluence.network/blog/nvidia-rtx-3090/)

---

## 3. NVIDIA DGX - Enterprise AI Platform

### DGX H100 - Flagship System

| Specifica | DGX H100 (8x H100 80GB) |
|-----------|-------------------------|
| **GPU** | 8x H100 SXM 80GB |
| **VRAM Totale** | 640GB |
| **Performance** | 32 PetaFLOPS FP8 |
| **Interconnect** | NVLink/NVSwitch 900 GB/s |
| **Prezzo Sistema** | **‚Ç¨360,000 - ‚Ç¨450,000** |
| **Prezzo Singola H100** | ‚Ç¨25,000 - ‚Ç¨40,000 |
| **TDP Sistema** | ~10.2 kW |
| **Form Factor** | Rack 8U |

### DGX Station - Deskside Workstation

**Nota**: DGX Station √® una linea separata (form factor desktop), non rack-mount come DGX H100. Modelli precedenti (A100-based) erano ~‚Ç¨100k-150k.

### Cloud Rental Alternative

**H100 80GB Cloud Pricing (2026):**
- **Jarvislabs**: $2.99/ora
- **Lambda Labs**: ~$3.50/ora
- **RunPod**: da $1.99/ora (Community Cloud)
- **Media mercato**: $2.85-3.50/ora

**Analisi Break-Even DGX H100:**
- Costo sistema: ‚Ç¨400k
- Cloud @$3/ora: ‚Ç¨2.70/ora
- Break-even ore: 148,148 ore = **16.9 anni di uso 24/7**
- **Conclusione**: Cloud SEMPRE pi√π economico per H100 (a meno di 24/7 uso intenso multi-anno)

### Quando Considerare DGX

**DGX √à per:**
- ‚úÖ Ricerca AI enterprise (universit√†, laboratori R&D)
- ‚úÖ Training modelli proprietari >100B parametri
- ‚úÖ Requisiti data sovereignty (dati non possono uscire on-premise)
- ‚úÖ Budget enterprise illimitato

**DGX NON √à per:**
- ‚ùå Self-hosting inference LLM 4B-30B (overkill massimo)
- ‚ùå Budget <‚Ç¨100k
- ‚ùå Uso occasionale (ROI mai raggiunto)

**Fonti:**
- [NVIDIA DGX H100 Deep Learning Console](https://viperatech.com/product/nvidia-dgx-h100-deep-learning-console-640gb-sxm5)
- [DGX H100 Price 2025 Cost & Specs](https://cyfuture.cloud/kb/gpu/nvidia-dgx-h100-price-2025-cost-specs-and-market-insights)
- [H100 GPU Cost 2025 Buy vs Rent](https://www.gmicloud.ai/blog/how-much-does-the-nvidia-h100-gpu-cost-in-2025-buy-vs-rent-analysis)
- [H100 Price Guide 2026](https://docs.jarvislabs.ai/blog/h100-price)

---

## 4. SERVER GPU CUSTOM - Build Your Own

### Componenti Richiesti & Budget

#### **Build Entry-Level (~‚Ç¨1,400-1,600)**

| Componente | Specifica | Prezzo |
|------------|-----------|--------|
| **GPU** | RTX 3060 12GB (usata) | ‚Ç¨350-400 |
| **CPU** | AMD Ryzen 5 5600 | ‚Ç¨150-180 |
| **Motherboard** | B550 ATX | ‚Ç¨100-130 |
| **RAM** | 32GB DDR4-3600 | ‚Ç¨90-110 |
| **Storage** | 1TB NVMe Gen3 | ‚Ç¨70-90 |
| **PSU** | 750W 80+ Gold | ‚Ç¨90-110 |
| **Case** | ATX Mid Tower | ‚Ç¨60-80 |
| **Cooling** | Air cooler mid-range | ‚Ç¨40-50 |
| **TOTALE** | | **‚Ç¨950-1,150** |

**Capacit√†**: Qwen3-4B ‚úÖ | 7B ‚úÖ | 13B ‚ö†Ô∏è (quantizzato) | 30B+ ‚ùå

---

#### **Build Mid-Range (~‚Ç¨2,000-2,400)**

| Componente | Specifica | Prezzo |
|------------|-----------|--------|
| **GPU** | **RTX 3090 24GB (usata)** | ‚Ç¨800-950 |
| **CPU** | AMD Ryzen 7 5800X3D | ‚Ç¨280-320 |
| **Motherboard** | X570 ATX | ‚Ç¨150-180 |
| **RAM** | 64GB DDR4-3600 | ‚Ç¨180-220 |
| **Storage** | 2TB NVMe Gen4 | ‚Ç¨150-180 |
| **PSU** | 850W 80+ Gold modular | ‚Ç¨120-150 |
| **Case** | ATX con buon airflow | ‚Ç¨80-120 |
| **Cooling** | 240mm AIO liquid | ‚Ç¨90-120 |
| **TOTALE** | | **‚Ç¨1,850-2,240** |

**Capacit√†**: Qwen3-4B ‚úÖ‚úÖ | 7B ‚úÖ‚úÖ | 13B ‚úÖ | 30B ‚úÖ (Q4) | 70B ‚ö†Ô∏è (Q4 aggressive)

üèÜ **RACCOMANDAZIONE PRIMARIA** - Miglior rapporto prezzo/performance/scalabilit√†

---

#### **Build High-End Single GPU (~‚Ç¨4,000-5,000)**

| Componente | Specifica | Prezzo |
|------------|-----------|--------|
| **GPU** | RTX 4090 24GB (nuova) | ‚Ç¨1,800-2,000 |
| **CPU** | AMD Ryzen 9 7950X3D | ‚Ç¨550-650 |
| **Motherboard** | X670E ATX | ‚Ç¨280-350 |
| **RAM** | 64GB DDR5-6000 | ‚Ç¨250-300 |
| **Storage** | 4TB NVMe Gen4 | ‚Ç¨280-350 |
| **PSU** | 1200W 80+ Platinum | ‚Ç¨200-250 |
| **Case** | Premium airflow | ‚Ç¨150-200 |
| **Cooling** | 360mm AIO liquid | ‚Ç¨150-200 |
| **TOTALE** | | **‚Ç¨3,660-4,300** |

**Capacit√†**: Come 3090 ma +40% velocit√† inference. Performance: ~112 token/s (8B), ~35 token/s (30B Q4)

---

#### **Build Dual-GPU (~‚Ç¨3,500-4,500) - MASSIMA VRAM**

| Componente | Specifica | Prezzo |
|------------|-----------|--------|
| **GPU** | 2x RTX 3090 24GB (usate) | ‚Ç¨1,600-1,900 |
| **CPU** | AMD Threadripper PRO 5965WX | ‚Ç¨2,000+ |
| **Motherboard** | TRX50 con x8/x8 PCIe | ‚Ç¨600-800 |
| **RAM** | 128GB DDR5 ECC | ‚Ç¨600-800 |
| **Storage** | 4TB NVMe Gen4 | ‚Ç¨280-350 |
| **PSU** | 1600W 80+ Platinum | ‚Ç¨350-450 |
| **Case** | Full Tower multi-GPU | ‚Ç¨200-300 |
| **Cooling** | Custom water loop | ‚Ç¨500-700 |
| **TOTALE** | | **‚Ç¨6,130-8,200** |

**Capacit√†**: 48GB VRAM totale ‚Üí modelli 70B Q4 comfortable, 120B possibile con aggressive quantization

**‚ö†Ô∏è ATTENZIONE**:
- x8/x8 vs x16/x4 PCIe = 23% differenza token generation speed
- Serve PSU 1200W+ (2x 350W GPU + 280W CPU + overhead)
- Thermal management critico (500W+ calore)
- Consumo elettrico: ~‚Ç¨1,500-3,000/anno @24/7

---

### Requisiti Tecnici Critici

#### **PSU (Power Supply)**

| GPU Setup | TDP GPU | PSU Minimo | PSU Raccomandato |
|-----------|---------|------------|------------------|
| 1x RTX 3090 | 350W | 750W | **850W 80+ Gold** |
| 1x RTX 4090 | 450W | 850W | **1000W 80+ Gold** |
| 2x RTX 3090 | 700W | 1200W | **1600W 80+ Platinum** |
| 2x RTX 4090 | 900W | 1500W | **1800W 80+ Platinum** |

**IMPORTANTE**: PSU deve gestire spike di potenza (transient load) che pu√≤ essere +20-30% del TDP dichiarato.

#### **Motherboard - Dual GPU**

**Per configurazioni dual-GPU LLM inference:**
- ‚úÖ **Chipset**: X670E (AMD) o Z790 (Intel) con supporto x8/x8 PCIe split
- ‚úÖ **PCIe lanes**: x8/x8 configuration (non x16/x4!) ‚Üí 23% perf difference
- ‚úÖ **Spacing**: 3-4 slot tra GPU per thermal management
- ‚ö†Ô∏è **CPU**: AMD EPYC/Threadripper o Intel Xeon per PCIe lanes sufficienti

**Motherboard consigliate dual-GPU:**
- ASUS Pro WS Z790-SAGE (Intel, ~‚Ç¨600)
- ASUS Pro WS X670E-ACE (AMD, ~‚Ç¨550)
- Supermicro H13SSL-N (AMD EPYC, ~‚Ç¨800)

#### **Cooling & Thermal Management**

**Single GPU (RTX 3090/4090):**
- Air cooling: 2x 140mm intake + 1x 140mm exhaust = sufficiente
- Target: <80¬∞C under load (>80¬∞C = thermal throttling = -20% performance)
- Rumore: ~45-55 dBA sotto carico (air cooling consumer)

**Dual GPU (2x 3090):**
- ‚ö†Ô∏è 500-700W calore concentrato in case
- Air cooling: 240mm intake **PER GPU** (4x 120mm front + 2x 140mm top)
- Liquid cooling raccomandato: -15-20¬∞C vs air, -10-15 dBA rumore
- Immersion cooling: -22¬∞C core temp vs air (68¬∞C vs 90¬∞C), -62.5% consumo ventole

**Costi cooling avanzato:**
- AIO 240mm: ‚Ç¨90-120
- AIO 360mm: ‚Ç¨150-200
- Custom water loop (CPU + 2 GPU): ‚Ç¨500-800
- Immersion cooling: ‚Ç¨2,000-5,000 (solo enterprise)

#### **Storage**

**VRAM vs Storage tradeoff:**
- Modello 70B Q4 = 35-40GB su disco
- Load time NVMe Gen3: ~8-12 secondi
- Load time SATA SSD: ~25-35 secondi
- Load time HDD: **2-4 minuti** ‚ùå MAI usare

**Raccomandazione**:
- Minimo: 1TB NVMe Gen3 (‚Ç¨70-90)
- Ideale: 2TB NVMe Gen4 (‚Ç¨150-180)
- Multi-modello: 4TB NVMe Gen4 (‚Ç¨280-350)

#### **RAM di Sistema**

**VRAM ‚â† System RAM**

| Use Case | RAM Minima | RAM Ideale |
|----------|------------|------------|
| Inference solo GPU | 16GB | 32GB |
| + CPU preprocessing | 32GB | 64GB |
| + Multi-modello caching | 64GB | 128GB |
| Production multi-user | 64GB | 128GB+ ECC |

**PERCH√â serve RAM sistema anche con GPU?**
- Model loading (temp buffer prima di GPU)
- Preprocessing/tokenization
- Request queuing & batching
- OS & serving framework overhead

**Fonti:**
- [Build Your Own AI PC 2026](https://techpurk.com/build-ai-pc-specs-2026-local-llms/)
- [LLM Server Hardware Guide 2026](https://novoserve.com/blog/how-to-run-an-llm-on-a-server-your-2026-llm-server-hardware-guide)
- [Best AMD Motherboards Dual GPU 2026](https://www.propelrc.com/best-amd-motherboards-for-dual-gpu-llm-builds/)
- [LLM Hardware Guide GPU RAM Storage](https://www.localai.computer/learn/llm-hardware-guide)

---

## 5. OPZIONI BUDGET - COSA COMPRARE PER INIZIARE

### Strategia Progressive Investment

```
FASE 1: Proof of Concept (‚Ç¨500-800)
  ‚Üì
FASE 2: Production Single-User (‚Ç¨1,500-2,200)
  ‚Üì
FASE 3: Scaling Multi-User (‚Ç¨3,500-5,000)
```

---

### **OPZIONE A: Minimo Viable (~‚Ç¨500-700)** üå±

**SCENARIO**: "Voglio solo testare self-hosting Qwen3-4B senza investire molto"

| Componente | Opzione | Prezzo |
|------------|---------|--------|
| **GPU** | RTX 3060 12GB (usata) | ‚Ç¨350-400 |
| **Resto del PC** | PC esistente albergo | ‚Ç¨0 |
| **Upgrade RAM** | +16GB DDR4 (se serve) | ‚Ç¨40-60 |
| **Upgrade PSU** | 650W (se serve) | ‚Ç¨60-80 |
| **TOTALE** | | **‚Ç¨450-540** |

**COSA PUOI FARE:**
- ‚úÖ Qwen3-4B Q4 @ ~40 token/s
- ‚úÖ 7B Q4 @ ~25 token/s
- ‚ö†Ô∏è 13B Q4 possibile ma lento
- ‚ùå 30B+ non possibile

**QUANDO HA SENSO:**
- Proof of concept prima di investimento grosso
- Budget molto limitato
- PC esistente gi√† disponibile con slot PCIe x16
- Uso occasionale/sviluppo

**‚ö†Ô∏è LIMITI:**
- 12GB VRAM = scaling limitato
- Performance non production-ready
- Non espandibile (devi cambiare GPU per upgrade)

---

### **OPZIONE B: Sweet Spot (~‚Ç¨1,800-2,200)** üèÜ **RACCOMANDATO**

**SCENARIO**: "Voglio self-hosting serio con scalabilit√† futura"

**BUILD COMPLETO:**

```
GPU:     RTX 3090 24GB (usata)             ‚Ç¨850
CPU:     AMD Ryzen 7 5800X                 ‚Ç¨280
MOBO:    MSI B550 Gaming Plus              ‚Ç¨130
RAM:     64GB DDR4-3600 (2x32GB)           ‚Ç¨200
SSD:     2TB NVMe Gen4 Samsung 980 Pro     ‚Ç¨170
PSU:     Corsair RM850x 80+ Gold           ‚Ç¨130
CASE:    Fractal Design Meshify 2          ‚Ç¨110
COOL:    Be Quiet Dark Rock Pro 4          ‚Ç¨80
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTALE:                                     ‚Ç¨1,950
```

**COSA PUOI FARE:**
- ‚úÖ‚úÖ Qwen3-4B Q4 @ ~80 token/s (latency <50ms)
- ‚úÖ‚úÖ 7B Q4 @ ~60 token/s
- ‚úÖ 13B Q4 @ ~35 token/s
- ‚úÖ 30B Q4 @ ~15 token/s
- ‚ö†Ô∏è 70B Q4 @ ~5 token/s (usabile ma lento)

**PERCH√â QUESTA BUILD:**
- **24GB VRAM**: Headroom per scaling fino 30B comfortable
- **RTX 3090 usata**: Miglior $/VRAM sul mercato (‚Ç¨35/GB vs ‚Ç¨80/GB di 4090)
- **64GB RAM**: Multi-modello caching, nessun bottleneck
- **Espandibilit√†**: Spazio per 2¬∞ GPU in futuro (se motherboard upgraded)
- **Silenzioso**: Dark Rock Pro 4 + Fractal case = <40 dBA idle

**UPGRADE PATH futuro:**
- +1x RTX 3090 usata (‚Ç¨850) ‚Üí 48GB totale ‚Üí 70B comfortable
- Swap RTX 3090 ‚Üí RTX 5090 32GB (2027) ‚Üí 32GB + architettura nuova

---

### **OPZIONE C: No-Compromise (~‚Ç¨3,800-4,500)** üöÄ

**SCENARIO**: "Voglio la migliore performance single-GPU oggi disponibile"

**BUILD COMPLETO:**

```
GPU:     RTX 4090 24GB (nuova)             ‚Ç¨1,900
CPU:     AMD Ryzen 9 7950X3D               ‚Ç¨600
MOBO:    ASUS ROG Strix X670E-E            ‚Ç¨350
RAM:     64GB DDR5-6000 CL30 (2x32GB)      ‚Ç¨280
SSD:     4TB NVMe Gen4 Samsung 990 Pro     ‚Ç¨320
PSU:     Corsair HX1200 80+ Platinum       ‚Ç¨240
CASE:    Lian Li O11 Dynamic EVO           ‚Ç¨180
COOL:    Arctic Liquid Freezer III 360     ‚Ç¨130
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTALE:                                     ‚Ç¨4,000
```

**COSA PUOI FARE:**
- ‚úÖ‚úÖ Qwen3-4B Q4 @ ~112 token/s (+40% vs 3090)
- ‚úÖ‚úÖ 7B Q4 @ ~85 token/s
- ‚úÖ‚úÖ 13B Q4 @ ~50 token/s
- ‚úÖ‚úÖ 30B Q4 @ ~22 token/s
- ‚úÖ 70B Q4 @ ~8 token/s

**PERCH√â QUESTA BUILD:**
- **RTX 4090**: Architettura Ada Lovelace + tensor cores 4¬∞ gen
- **7950X3D**: Miglior CPU gaming/AI (V-Cache utile per inference)
- **DDR5-6000**: Bandwidth per preprocessing veloce
- **Cooling premium**: Thermal throttling impossibile, rumore minimo
- **Futuro-proof**: DDR5, PCIe 5.0, supporto GPU next-gen

**‚ö†Ô∏è QUANDO NON HA SENSO:**
- Budget limitato (3090 usata = 80% performance a 40% del costo)
- Uso occasionale (ROI mai raggiunto)
- Modelli <13B (overhead architettura non sfruttata)

---

### **OPZIONE D: Dual-GPU Scaling (~‚Ç¨3,200-4,000)** üí™

**SCENARIO**: "Voglio servire modelli 70B+ comfortable o multi-user"

**BUILD COMPLETO:**

```
GPU:     2x RTX 3090 24GB (usate)          ‚Ç¨1,700
CPU:     AMD Ryzen 9 7950X                 ‚Ç¨550
MOBO:    ASUS Pro WS X670E-ACE             ‚Ç¨520
RAM:     128GB DDR5-5600 ECC (4x32GB)      ‚Ç¨700
SSD:     4TB NVMe Gen4                     ‚Ç¨320
PSU:     Corsair AX1600i 80+ Titanium      ‚Ç¨450
CASE:    Fractal Design Define 7 XL        ‚Ç¨200
COOL:    2x Arctic Liquid Freezer II 280   ‚Ç¨220
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTALE:                                     ‚Ç¨4,660
```

**COSA PUOI FARE:**
- **48GB VRAM Totale** via tensor parallelism
- ‚úÖ 70B Q4 @ ~12-15 token/s (comfortable)
- ‚úÖ 120B Q4 @ ~5-8 token/s (possibile con aggressive quant)
- ‚úÖ Multi-user: 4-8 utenti concurrent 13B model

**‚ö†Ô∏è COMPLESSIT√Ä:**
- Serve software tensor parallelism (vLLM, TensorRT-LLM, DeepSpeed)
- PSU 1600W necessaria (2x 350W GPU + overhead)
- Cooling critico (500W+ calore)
- Rumore: 50-60 dBA sotto carico (anche con liquid)
- Consumo elettrico: ~‚Ç¨1,200-2,400/anno @24/7

**QUANDO HA SENSO:**
- Modelli >30B sono requirement
- Multi-user production (>5 concurrent)
- Budget ‚Ç¨4-5k disponibile
- Competenza tecnica configurazione

---

### **OPZIONE E: Jetson Entry (~‚Ç¨250-500)** üêú

**SCENARIO**: "Voglio testare edge inference con consumo bassissimo"

| Modello | VRAM | Prezzo | TDP | Performance |
|---------|------|--------|-----|-------------|
| Jetson Orin Nano Super | 8GB | **‚Ç¨249** | 25W | ~15 token/s (4B) |
| Jetson AGX Orin 32GB | 32GB | ‚Ç¨999 | 60W | ~25 token/s (4B) |

**COSA PUOI FARE:**
- ‚úÖ Qwen3-4B Q4 @ ~15-25 token/s
- ‚ö†Ô∏è 7B Q4 possibile ma lento (~8-12 token/s)
- ‚ùå 13B+ non raccomandato

**QUANDO HA SENSO:**
- Proof of concept edge deployment
- Consumo energetico critico (<30W vs 400W GPU desktop)
- Form factor compatto necessario
- Budget minimo

**‚ö†Ô∏è LIMITI:**
- Performance 1/5 rispetto a RTX 3090
- Non espandibile (memoria saldata)
- Limitato a modelli <13B

---

### TABELLA COMPARATIVA OPZIONI

| Opzione | Budget | VRAM | Performance 4B | Scaling | Consumo 24/7/anno | ROI vs Cloud |
|---------|--------|------|----------------|---------|-------------------|--------------|
| **A: Minimo** | ‚Ç¨500 | 12GB | ~40 tok/s | ‚ö†Ô∏è Limitato | ‚Ç¨400-600 | 6-8 mesi |
| **B: Sweet Spot** üèÜ | ‚Ç¨1,950 | 24GB | ~80 tok/s | ‚úÖ Ottimo | ‚Ç¨650-900 | 8-12 mesi |
| **C: No-Compromise** | ‚Ç¨4,000 | 24GB | ~112 tok/s | ‚úÖ Ottimo | ‚Ç¨750-1,100 | 18-24 mesi |
| **D: Dual-GPU** | ‚Ç¨4,660 | 48GB | ~80 tok/s | ‚úÖ‚úÖ Max | ‚Ç¨1,200-2,400 | 24-36 mesi |
| **E: Jetson** | ‚Ç¨250 | 8GB | ~15 tok/s | ‚ùå Nullo | ‚Ç¨35-60 | 3-4 mesi |

**RACCOMANDAZIONE FINALE**: **Opzione B (RTX 3090 Sweet Spot)** per 95% dei casi.

**Fonti:**
- [Best Budget GPUs for AI 2026](https://www.fluence.network/blog/best-budget-gpus/)
- [Top 7 Best Budget GPUs for AI 2026](https://techtactician.com/best-budget-gpus-for-local-ai-workflows/)
- [RTX 3090 Price Tracker Jan 2026](https://bestvaluegpu.com/history/new-and-used-rtx-3090-price-history-and-specs/)

---

## 6. COSTI TOTALI STIMATI

### Costi Iniziali (CAPEX)

| Categoria | Budget Basso | Budget Medio | Budget Alto |
|-----------|--------------|--------------|-------------|
| **Hardware** | ‚Ç¨500-700 | ‚Ç¨1,800-2,200 | ‚Ç¨4,000-5,000 |
| **Accessori** | ‚Ç¨50-100 | ‚Ç¨100-200 | ‚Ç¨200-300 |
| **Software/License** | ‚Ç¨0 (FOSS) | ‚Ç¨0 (FOSS) | ‚Ç¨0-500 |
| **Setup/Testing** | ‚Ç¨0 (DIY) | ‚Ç¨0 (DIY) | ‚Ç¨200-500 (pro) |
| **TOTALE CAPEX** | **‚Ç¨550-800** | **‚Ç¨1,900-2,600** | **‚Ç¨4,400-6,300** |

*Note: Software FOSS = Ollama, vLLM, FastAPI, TensorRT-LLM (tutti free)*

---

### Costi Ricorrenti (OPEX) - Annuali

#### **Elettricit√† (Componente Maggiore)**

**Prezzi elettricit√† Europa 2026:**
- Italia: ‚Ç¨0.25-0.35/kWh
- Media UE: ‚Ç¨0.22-0.30/kWh
- Range utilizzato: **‚Ç¨0.25-0.40/kWh**

**Consumi 24/7 con utilizzo variabile:**

| Scenario | GPU | Load% | kWh/giorno | ‚Ç¨/anno @‚Ç¨0.30 | ‚Ç¨/anno @‚Ç¨0.40 |
|----------|-----|-------|------------|---------------|---------------|
| **Idle** | RTX 3090 | 10% | 1.0 kWh | ‚Ç¨110 | ‚Ç¨146 |
| **Light** | RTX 3090 | 30% | 3.0 kWh | ‚Ç¨330 | ‚Ç¨440 |
| **Medium** | RTX 3090 | 50% | 5.0 kWh | ‚Ç¨550 | ‚Ç¨730 |
| **Heavy** | RTX 3090 | 80% | 8.0 kWh | ‚Ç¨876 | ‚Ç¨1,168 |
| **Full 24/7** | RTX 3090 | 100% | 10.8 kWh | **‚Ç¨1,186** | **‚Ç¨1,580** |
| | | | | | |
| **Medium** | RTX 4090 | 50% | 5.4 kWh | ‚Ç¨591 | ‚Ç¨788 |
| **Full 24/7** | RTX 4090 | 100% | 10.8 kWh | ‚Ç¨1,186 | ‚Ç¨1,580 |
| | | | | | |
| **Medium** | 2x RTX 3090 | 50% | 10.0 kWh | ‚Ç¨1,095 | ‚Ç¨1,460 |
| **Full 24/7** | 2x RTX 3090 | 100% | 20.0 kWh | **‚Ç¨2,190** | **‚Ç¨2,920** |

**STIMA REALISTICA per self-hosting albergo:**
- **Uso tipico**: 40-60% load medio (inference on-demand, non 24/7 full)
- **RTX 3090 costo elettricit√†**: **‚Ç¨600-900/anno**
- **Dual RTX 3090 costo elettricit√†**: **‚Ç¨1,200-1,800/anno**

#### **Altri Costi Ricorrenti**

| Voce | Costo Annuale |
|------|---------------|
| **Elettricit√†** | ‚Ç¨600-900 (single GPU) |
| **Cooling extra** | ‚Ç¨0-100 (AC se estate) |
| **Manutenzione** | ‚Ç¨50-150 (pulizia, thermal paste) |
| **Backup/Storage** | ‚Ç¨50-100 (dischi esterni) |
| **Network** | ‚Ç¨0 (gi√† disponibile) |
| **Software updates** | ‚Ç¨0 (FOSS) |
| **TOTALE OPEX/anno** | **‚Ç¨700-1,250** |

---

### Total Cost of Ownership (TCO) - 3 Anni

| Scenario | CAPEX | OPEX 3yr | TCO 3yr | TCO/mese |
|----------|-------|----------|---------|----------|
| **Budget (RTX 3060)** | ‚Ç¨700 | ‚Ç¨1,800 | **‚Ç¨2,500** | ‚Ç¨69 |
| **Sweet Spot (RTX 3090)** üèÜ | ‚Ç¨2,000 | ‚Ç¨2,700 | **‚Ç¨4,700** | ‚Ç¨131 |
| **High-End (RTX 4090)** | ‚Ç¨4,000 | ‚Ç¨3,000 | **‚Ç¨7,000** | ‚Ç¨194 |
| **Dual GPU (2x3090)** | ‚Ç¨4,700 | ‚Ç¨4,500 | **‚Ç¨9,200** | ‚Ç¨256 |

**Fonti:**
- [RTX 4090 Power Consumption Guide](https://www.ecoenergygeek.com/rtx-4090-power-consumption/)
- [H100 GPU Electricity Cost Analysis](https://news.ycombinator.com/item?id=38262665)
- [GPU Server Thermal Management Costs](https://www.electronics-cooling.com/2017/02/thermal-management-gpu-enabled-servers-data-centers/)
- [Costs of Deploying AI Energy Cooling](https://www.exxactcorp.com/blog/hpc/the-costs-of-deploying-ai-energy-cooling-management)

---

## 7. BREAK-EVEN ANALYSIS - Cloud vs Self-Hosting

### Cloud Pricing Benchmark (2026)

**GPU Cloud - Prezzi Orari:**

| Provider | RTX 4090 | RTX 3090 | H100 80GB | Note |
|----------|----------|----------|-----------|------|
| **Vast.ai** | $0.34/hr | $0.25/hr | $2.50/hr | Marketplace, prezzi variabili |
| **RunPod Community** | $0.34/hr | $0.28/hr | $1.99/hr | Per-second billing |
| **RunPod Secure** | $0.61/hr | - | $2.39/hr | Enterprise features |
| **Lambda Labs** | - | - | $3.50/hr | Transparent pricing |
| **Jarvislabs** | - | - | $2.99/hr | Stabile, affidabile |

**MEDIA utilizzata per calcoli:**
- RTX 3090-equivalent: **$0.30/ora** (‚Ç¨0.27/ora)
- RTX 4090-equivalent: **$0.40/ora** (‚Ç¨0.36/ora)

---

### Scenari di Utilizzo & Break-Even

#### **SCENARIO 1: Uso Light (2-4 ore/giorno)**

**Utilizzo**: Sviluppo, testing, chatbot personale uso ufficio ore lavoro

| Metrica | Cloud | Self-Hosting (RTX 3090) |
|---------|-------|-------------------------|
| **Ore/giorno** | 3 ore | 3 ore @ 50% load medio |
| **Costo/giorno** | ‚Ç¨0.81 | ‚Ç¨0.41 (elettricit√†) |
| **Costo/mese** | ‚Ç¨24 | ‚Ç¨12 + ‚Ç¨8 ammortamento = ‚Ç¨20 |
| **Costo/anno** | **‚Ç¨292** | **‚Ç¨240** |
| **Break-even** | - | **8 mesi** |

**VERDETTO**: Self-hosting leggermente conveniente, ma margine sottile. Cloud pi√π flessibile.

---

#### **SCENARIO 2: Uso Medium (8-12 ore/giorno)** üèÜ

**Utilizzo**: Production chatbot interno albergo, assistente staff, uso intensivo giornaliero

| Metrica | Cloud | Self-Hosting (RTX 3090) |
|---------|-------|-------------------------|
| **Ore/giorno** | 10 ore | 10 ore @ 60% load medio |
| **Costo/giorno** | ‚Ç¨2.70 | ‚Ç¨0.55 (elettricit√†) |
| **Costo/mese** | ‚Ç¨81 | ‚Ç¨17 + ‚Ç¨55 ammortamento = ‚Ç¨72 (primi 3 anni) |
| **Costo/anno** | **‚Ç¨986** | **‚Ç¨267** |
| **Break-even** | - | **2.4 mesi** ‚úÖ |

**VERDETTO**: Self-hosting **NETTAMENTE conveniente**. ROI rapidissimo.

---

#### **SCENARIO 3: Uso Heavy (24/7 production)**

**Utilizzo**: Servizio sempre attivo, multi-utente, high availability

| Metrica | Cloud | Self-Hosting (RTX 3090) |
|---------|-------|-------------------------|
| **Ore/giorno** | 24 ore | 24 ore @ 70% load medio |
| **Costo/giorno** | ‚Ç¨6.48 | ‚Ç¨0.69 (elettricit√†) |
| **Costo/mese** | ‚Ç¨194 | ‚Ç¨21 + ‚Ç¨55 ammortamento = ‚Ç¨76 |
| **Costo/anno** | **‚Ç¨2,366** | **‚Ç¨912** |
| **Break-even** | - | **1.0 mese** ‚úÖ‚úÖ |

**VERDETTO**: Self-hosting **ULTRA-CONVENIENTE**. Cloud costa 2.6x di pi√π annualmente.

---

### Token-Based Break-Even Analysis

**Ricerca accademica (arXiv 2509.18101)** indica:

> "A private LLM starts to pay off when you process over **2 million tokens a day** or require strict compliance like HIPAA or PCI"

**Calcolo Tokens/Giorno:**

| Scenario | Richieste/giorno | Token/richiesta | Token/giorno | Break-even? |
|----------|------------------|-----------------|--------------|-------------|
| **Light** | 50 | 500 | 25k | ‚ùå Cloud |
| **Medium** | 300 | 800 | 240k | ‚ö†Ô∏è Borderline |
| **Heavy** | 1,000 | 1,200 | 1.2M | ‚ö†Ô∏è Vicino |
| **Production** | 2,500 | 1,000 | **2.5M** | ‚úÖ Self-host |
| **Enterprise** | 10,000 | 800 | **8M** | ‚úÖ‚úÖ Self-host |

---

### Utilizzo Overtime - Hidden Costs

**Cloud "creep cost":**

| Anno | Costo Cloud @10hr/day | Costo Self-Hosting | Differenza Cumulativa |
|------|----------------------|--------------------|-----------------------|
| **Anno 1** | ‚Ç¨986 | ‚Ç¨2,000 (CAPEX) + ‚Ç¨267 = ‚Ç¨2,267 | -‚Ç¨1,281 (cloud ahead) |
| **Anno 2** | ‚Ç¨986 | ‚Ç¨267 | -‚Ç¨295 (self-host ahead!) |
| **Anno 3** | ‚Ç¨986 | ‚Ç¨267 | -‚Ç¨1,014 |
| **Anno 4** | ‚Ç¨986 | ‚Ç¨267 | -‚Ç¨1,733 |
| **Anno 5** | ‚Ç¨986 | ‚Ç¨267 | -‚Ç¨2,452 |

**Break-even point**: **14 mesi** per scenario medium usage (10hr/day)

---

### Fattori Qualitativi (Non-Monetary)

| Fattore | Cloud | Self-Hosting |
|---------|-------|--------------|
| **Privacy/Data Sovereignty** | ‚ö†Ô∏è Dati su server terzi | ‚úÖ Controllo totale |
| **Compliance (HIPAA/PCI)** | ‚ö†Ô∏è Certifications needed | ‚úÖ On-premise compliant |
| **Latency** | ‚ö†Ô∏è 50-150ms (network) | ‚úÖ <10ms (local) |
| **Customization** | ‚ö†Ô∏è Limitata | ‚úÖ Totale |
| **Scalability** | ‚úÖ Immediata | ‚ö†Ô∏è Richiede HW upgrade |
| **Maintenance** | ‚úÖ Zero effort | ‚ö†Ô∏è DIY o contratto |
| **Uptime SLA** | ‚úÖ 99.9% garantito | ‚ö†Ô∏è Dipende da te |

---

### RACCOMANDAZIONE BREAK-EVEN

**SCEGLI CLOUD SE:**
- ‚úÖ Uso <4 ore/giorno (<‚Ç¨400/anno)
- ‚úÖ Spike imprevedibili (scaling elastico)
- ‚úÖ Non vuoi gestire hardware
- ‚úÖ Testing/PoC breve termine (<6 mesi)
- ‚úÖ Serve GPU diverse per confronto (H100, A100, etc)

**SCEGLI SELF-HOSTING SE:**
- ‚úÖ Uso >8 ore/giorno (>‚Ç¨800/anno cloud)
- ‚úÖ >2M token/giorno processati
- ‚úÖ Requisiti compliance (GDPR strict, HIPAA, PCI-DSS)
- ‚úÖ Privacy critica (dati sensibili clienti albergo)
- ‚úÖ Latency <20ms richiesta
- ‚úÖ Orizzonte >2 anni utilizzo
- ‚úÖ Competenza tecnica disponibile

**HYBRID APPROACH (BEST):**
- üèÜ Self-hosting per baseline load (modelli <13B 24/7)
- üèÜ Cloud per spike/peak (modelli >30B occasionali)
- üèÜ Cloud per A/B testing nuove architetture
- üèÜ Self-hosting per production, cloud per staging

**PER ALBERGO con Qwen3-4B:**
- Uso stimato: 8-12 ore/giorno (orario reception + back-office)
- Token/giorno: ~500k-1M (assistenza clienti, email, scheduling)
- **RACCOMANDAZIONE**: Self-hosting RTX 3090 (~‚Ç¨2k) = **break-even 8-12 mesi**, poi saving ‚Ç¨700-1,200/anno

**Fonti:**
- [Cost Analysis LLMs Cloud vs Self-Hosted](https://medium.com/artefact-engineering-and-data-science/llms-deployment-a-practical-cost-analysis-e0c1b8eb08ca)
- [Cloud vs On-Prem LLMs Long-Term Cost](https://latitude-blog.ghost.io/blog/cloud-vs-on-prem-llms-long-term-cost-analysis/)
- [Cost-Benefit Analysis On-Premise LLM (arXiv)](https://arxiv.org/html/2509.18101v1)
- [LLM Total Cost of Ownership 2025](https://www.ptolemay.com/post/llm-total-cost-of-ownership)
- [GPU Cloud Pricing Comparison 2026](https://getdeploying.com/gpus)
- [RunPod vs Vast.ai GPU Pricing](https://computeprices.com/compare/runpod-vs-vast)

---

## 8. PRO / CONTRO - Self-Hosting vs Cloud

### Self-Hosting Hardware Proprio

#### ‚úÖ PRO

**Economico (lungo termine):**
- Break-even 8-24 mesi per uso medium-heavy
- Saving 30-50% su 3 anni vs cloud continuo
- Nessun costo per ora non utilizzata (cloud fattura anche idle)

**Privacy & Controllo:**
- Dati GDPR-sensitive restano on-premise (clienti albergo)
- Zero rischio data breach provider terzi
- Compliance HIPAA/PCI-DSS semplificata
- Audit trail completo locale

**Performance & Latency:**
- <10ms latency (vs 50-150ms cloud)
- Nessun throttling/rate limiting
- Bandwidth illimitata (LAN gigabit)
- Nessuna "noisy neighbor" (cloud shared resources)

**Customization:**
- Controllo totale stack software (kernel, drivers, CUDA version)
- Hardware tuning estremo (overclocking, undervolting)
- Sperimentazione architetture custom

**Affidabilit√† (controllo):**
- Nessuna dipendenza da uptime provider esterno
- Nessun lock-in vendor
- Hardware sostituibile immediatamente (spare parts)

#### ‚ùå CONTRO

**Costo Iniziale Alto:**
- CAPEX ‚Ç¨2k-5k upfront (vs ‚Ç¨0 cloud)
- Cash flow negativo primi 8-24 mesi
- Rischio obsolescenza hardware (3-5 anni)

**Manutenzione & Gestione:**
- Serve competenza tecnica (Linux, CUDA, networking)
- Pulizia fisica, thermal paste, monitoring
- Debugging hardware/software in autonomia
- 40-60% hidden costs per maintenance (ricerca industry)

**Scalabilit√† Limitata:**
- Upgrade richiede acquisto nuovo hardware
- Dual-GPU = raddoppio complessit√†
- Nessuna elasticit√† (spike ‚Üí hardware idle resto del tempo)

**Cooling & Rumore:**
- 45-60 dBA sotto carico (air cooling)
- Richiede ambiente dedicato (non reception)
- Calore: 350-700W = stanza calda estate
- AC potrebbe servire ‚Üí costo elettricit√† extra

**Elettricit√†:**
- Costo ricorrente ‚Ç¨600-1,800/anno
- Dipende da tariffe locali (rischio aumenti)
- Impact ambientale (carbon footprint)

**Nessun SLA:**
- Hardware failure = downtime fino a riparazione/sostituzione
- Nessun supporto 24/7
- Rischio fulmine/surge (serve UPS)

---

### Cloud GPU (Vast.ai, RunPod, Lambda)

#### ‚úÖ PRO

**Zero CAPEX:**
- Costo iniziale ‚Ç¨0
- Pay-as-you-go puro
- Cash flow positivo da subito (OpEx, non CapEx)

**Scalabilit√† Elastica:**
- Scaling up/down in <5 minuti
- A100/H100 on-demand per spike
- Multi-GPU/multi-region geografica
- Testing architetture diverse senza acquisto

**Zero Manutenzione:**
- Hardware failure = provider problem
- Software updates automatiche
- Monitoring incluso
- Support 24/7 enterprise tier

**SLA & Uptime:**
- 99.9% uptime garantito (contract)
- Ridondanza geografica
- Backup automatici

**Flessibilit√†:**
- Cambi GPU in 2 click (RTX 4090 ‚Üí H100)
- Testing multi-modello parallelo
- Spegni quando non usi = ‚Ç¨0

#### ‚ùå CONTRO

**Costo Continuo:**
- Creep cost: ‚Ç¨1k-3k/anno per uso medium
- Nessun break-even (costo infinito over time)
- Surprise bills se lasci istanza accesa

**Privacy & Data Sovereignty:**
- Dati su server terzi (USA/UE mix)
- GDPR compliance complessa
- Rischio data breach provider
- Logs/telemetry inviati a provider

**Performance Variabile:**
- Network latency 50-150ms
- "Noisy neighbor" su cloud shared
- Throttling durante peak demand
- Bandwidth limits/costs

**Vendor Lock-In:**
- Switching provider = rifare setup
- Formati storage proprietari
- API/SDK specific per piattaforma

**Costi Nascosti:**
- Egress bandwidth (download modelli)
- Storage persistente
- Premium GPUs/regions
- Enterprise support

---

### HYBRID APPROACH - Best of Both Worlds üèÜ

**STRATEGIA RACCOMANDATA per Albergo:**

```
BASELINE (24/7):
  Self-Hosting RTX 3090 ‚Üí Qwen3-4B, 7B, 13B
  Costo: ‚Ç¨2k CAPEX + ‚Ç¨700/anno OpEx

SPIKE/TESTING:
  Cloud RunPod/Vast.ai ‚Üí Modelli 30B+, A/B testing
  Costo: ~‚Ç¨50-150/mese (uso occasionale)

DISASTER RECOVERY:
  Cloud backup quando HW locale down
  Costo: ~‚Ç¨20/mese (standby)
```

**Benefici Hybrid:**
- ‚úÖ 80% workload su hardware proprio (cheap)
- ‚úÖ 20% spike su cloud (flessibilit√†)
- ‚úÖ DR/failover automatico
- ‚úÖ A/B testing nuovi modelli senza rischio
- ‚úÖ TCO ottimizzato (-40% vs pure cloud)

**Fonti:**
- [LLM as a Service vs Self-Hosted Analysis](https://www.binadox.com/blog/modern-digital-area/llm-as-a-service-vs-self-hosted-cost-and-performance-analysis/)
- [Commercial vs Self-Hosted LLMs](https://www.iguazio.com/blog/commercial-vs-self-hosted-llms/)
- [68% of companies use hybrid AI hosting](https://latitude-blog.ghost.io/blog/cloud-vs-on-prem-llms-long-term-cost-analysis/)

---

## 9. MERCATO GPU USATO - Opportunit√† & Rischi

### Prezzi Mercato Usato (Gennaio 2026)

| GPU | VRAM | Prezzo Nuovo | Prezzo Usato | Risparmio | Disponibilit√† |
|-----|------|--------------|--------------|-----------|---------------|
| **RTX 3090** | 24GB | ‚Ç¨1,100-1,300 | **‚Ç¨800-1,000** | 27-38% | üü¢ Alta (eBay, forum) |
| **RTX 3090 Ti** | 24GB | ‚Ç¨1,300-1,500 | ‚Ç¨1,000-1,300 | 23-31% | üü° Media |
| **RTX 4090** | 24GB | ‚Ç¨1,800-2,000 | ‚Ç¨1,600-1,900 | 11-20% | üü° Media |
| **RTX 3080 Ti** | 12GB | ‚Ç¨800-900 | ‚Ç¨500-650 | 28-44% | üü¢ Alta |
| **RTX 3060 12GB** | 12GB | ‚Ç¨400-500 | ‚Ç¨300-400 | 20-33% | üü¢ Alta |
| **RTX A5000** | 24GB | ‚Ç¨2,500 | ‚Ç¨1,500-2,000 | 20-40% | üî¥ Bassa (enterprise) |

### Perch√© RTX 3090 Usate Convenienti?

**Mining Boom 2021-2022 = Flooding Mercato 2023-2026:**
- Ethereum merge ‚Üí mining non pi√π profittevole
- Migliaia di GPU mining flood mercato usato
- Prezzo 3090 calato da ‚Ç¨2,000 (2021) ‚Üí ‚Ç¨800-1,000 (2026)

**VRAM Premium:**
- 24GB VRAM = AI/ML use case ancora rilevante
- "Extra VRAM helps in AI and productivity workloads" - utenti forum
- Domanda stabile mantiene prezzi sopra ‚Ç¨800 (non crollano come 3080 12GB)

### Dove Comprare Usato

| Piattaforma | Vantaggi | Svantaggi | Buyer Protection |
|-------------|----------|-----------|------------------|
| **eBay** | Alta disponibilit√†, prezzi competitivi | Rischio scam, condizioni variabili | ‚úÖ Money Back (60gg) |
| **Subito.it** | Ritiro persona, ispezione fisica | Prezzi spesso alti | ‚ùå Nessuna (privato) |
| **Amazon Warehouse** | Reso facile, garanzia Amazon | Stock limitato GPU | ‚úÖ 30gg reso |
| **Forum HW (hwupgrade)** | Community affidabile, prezzi buoni | Richiede reputazione | ‚ö†Ô∏è Dipende da venditore |
| **Retailer Ricondizionati** | Garanzia 1-2 anni | Prezzi ~10% sotto nuovo | ‚úÖ Garanzia ufficiale |

### Checklist Acquisto Usato

**PRIMA dell'acquisto:**
- ‚úÖ Chiedi foto timestamp (GPU + username scritto su carta)
- ‚úÖ Verifica serial number NVIDIA (anti-stolen)
- ‚úÖ Chiedi storia: gaming, mining, rendering? (mining = peggio)
- ‚úÖ Controlla feedback venditore (>98% positivo)

**Durante test (se ritiro persona):**
- ‚úÖ Porta laptop con GPU-Z installato
- ‚úÖ Verifica VRAM completa riconosciuta (24GB, no 23.5GB = problema)
- ‚úÖ Stress test 15min (FurMark o 3DMark)
- ‚úÖ Controlla temperature (<80¬∞C sotto load)
- ‚úÖ Ascolta rumori strani (coil whine ok, grinding NO)
- ‚úÖ Ispeziona fisicamente: backplate integra, PCB no bruciature, fan smooth

**Dopo acquisto:**
- ‚úÖ Test 48h intenso (catch DOA entro return period)
- ‚úÖ Benchmark vs stock (UserBenchmark, 3DMark)
- ‚úÖ Memtest GPU (OCCT VRAM test 1hr)

### Red Flags - NON COMPRARE SE

- üö© Venditore <95% feedback o account nuovo
- üö© Prezzo troppo buono (‚Ç¨600 per 3090 = scam)
- üö© "Non testata, venduta come √®" (dead GPU)
- üö© Modifiche fisiche (waterblock custom, backplate rimossa)
- üö© No fattura originale (possibile stolen)
- üö© Rifiuta test pre-acquisto (nasconde problema)

### Garanzia & RMA

**NVIDIA Garanzia Originale:**
- Consumer GPU (RTX serie): **3 anni** dalla data fattura originale
- Workstation (RTX A/6000): **3 anni** trasferibile
- **IMPORTANTE**: Serve fattura originale per RMA

**Cosa fare:**
- ‚úÖ Chiedi sempre fattura originale scannerizzata
- ‚úÖ Calcola garanzia residua (< 1 anno = sconto maggiore)
- ‚úÖ Register GPU su NVIDIA account appena acquistata
- ‚ö†Ô∏è Mining usage INVALIDA garanzia (se provato)

### Caso Studio - RTX 3090 Mining

**Domanda**: Le GPU mining sono affidabili?

**Risposta bilanciata:**

**PRO mining GPU:**
- Undervolt costante (mining efficiente = meno voltage = meno stress)
- Temperature stabili (non thermal cycling come gaming)
- No shock fisici (server rack, non LAN party)

**CONTRO mining GPU:**
- 24/7 uso per mesi/anni = wear elevato
- Fan a 100% = bearing usurato (rumore, failure)
- Thermal paste secca (sostituire SUBITO)
- VRAM clock alto continuo (degrado memory controller)

**RACCOMANDAZIONE:**
- ‚úÖ Accetta mining SE: prezzo sconto >30%, garanzia residua >1 anno, test approfondito
- ‚ùå Evita SE: prezzo vicino a non-mining, no garanzia, venditore opaco

### Value Picks Usato (Gennaio 2026)

| Pick | GPU | Prezzo Target | VRAM | Perch√© |
|------|-----|---------------|------|--------|
| ü•á **Best Value** | RTX 3090 | ‚Ç¨800-900 | 24GB | Miglior $/VRAM, scaling ottimo |
| ü•à **Budget King** | RTX 3060 12GB | ‚Ç¨300-350 | 12GB | Entry-level 4B-7B modelli |
| ü•â **Workstation** | RTX A5000 | ‚Ç¨1,500-1,800 | 24GB | ECC, blower cooling, garanzia trasferibile |
| ‚ö†Ô∏è **Risky Bet** | RTX 3080 Ti | ‚Ç¨500-600 | 12GB | Buon prezzo MA 12GB limitante |

**Fonti:**
- [RTX 3090 Price Tracker January 2026](https://bestvaluegpu.com/history/new-and-used-rtx-3090-price-history-and-specs/)
- [eBay NVIDIA GeForce RTX 3090 24GB](https://www.ebay.com/b/NVIDIA-GeForce-RTX-3090-24GB-GDDR6-Graphics-Cards/27386/bn_7117810176)
- [RTX 4090 Price Tracker January 2026](https://bestvaluegpu.com/history/new-and-used-rtx-4090-price-history-and-specs/)
- [Why are 3090s still so expensive? (Forum)](https://hardforum.com/threads/why-are-3090s-still-so-expensive.2039366/)

---

## 10. CONSIDERAZIONI PRATICHE - Location Albergo

### Infrastruttura Esistente Albergo

**Asset disponibili:**
- ‚úÖ PC Windows esistente (possibile upgrade GPU)
- ‚úÖ Network switch & firewall gi√† configurati
- ‚úÖ Infrastruttura elettrica stabile
- ‚úÖ Spazio dedicato (back-office/server room?)
- ‚ö†Ô∏è AC/climatizzazione da verificare

### Installazione Fisica

#### **Opzione 1: Upgrade PC Esistente** (‚Ç¨500-1,000)

**PRO:**
- Riutilizzo case, PSU (se sufficiente), storage
- Zero spazio aggiuntivo
- Setup veloce (slot GPU + drivers)

**CONTRO:**
- PSU potrebbe servire upgrade (750W+)
- Motherboard compatibility (slot PCIe x16?)
- Thermal management caso non ottimizzato per GPU

**Checklist:**
```
1. Verifica PSU wattage (750W+ per RTX 3090)
2. Misura slot PCIe disponibili (GPU 3090 = 2.5-3 slot)
3. Check clearance case (3090 = 31-33cm lunghezza)
4. Airflow: almeno 2x 120mm intake
```

---

#### **Opzione 2: Build Dedicata** (‚Ç¨1,800-2,200)

**PRO:**
- Ottimizzato per AI workload
- Cooling adeguato
- Nessuna interferenza con PC produzione esistente

**CONTRO:**
- Serve spazio fisico dedicato
- Costo pieno build
- Gestione 2 macchine separate

**Location ideale:**
- Server room/back-office con AC
- Lontano da reception (rumore)
- Access rete cablata gigabit
- Vicino a UPS (protezione spike)

---

### Cooling & Rumore Management

**Rumore GPU sotto carico:**

| Scenario | Cooling | dBA @ 1m | Percezione |
|----------|---------|----------|------------|
| **Idle** | Air standard | 25-30 dBA | Quasi silenzioso |
| **Light load** | Air standard | 35-40 dBA | PC ufficio normale |
| **Medium load** | Air standard | 45-50 dBA | Aspirapolvere lontano |
| **Heavy load** | Air standard | 55-60 dBA | ‚ö†Ô∏è Disturbante |
| **Heavy load** | Liquid AIO | 40-45 dBA | Accettabile |
| **24/7 optimized** | Custom water | 30-35 dBA | Silenzioso |

**RACCOMANDAZIONE Albergo:**
- ‚úÖ Posiziona in server room/back-office (non reception)
- ‚úÖ Se air cooling: case con sound dampening (Fractal Define series)
- ‚úÖ Se budget: AIO 240mm liquid (RTX 3090 compatible)
- ‚ö†Ô∏è Evita blower-style GPU se vicino staff (A5000, RTX 6000)

---

### Gestione Calore

**Output termico:**

| GPU | TDP | BTU/hr | Equivalente |
|-----|-----|--------|-------------|
| RTX 3090 | 350W | 1,195 BTU | Stufa piccola |
| RTX 4090 | 450W | 1,536 BTU | Forno aperto |
| 2x RTX 3090 | 700W | 2,390 BTU | Stufetta media |

**Impatto stanza:**
- Stanza 10m¬≤ + RTX 3090 24/7 = +3-5¬∞C temperatura ambiente
- Estate (30¬∞C esterno) + GPU = 35-38¬∞C stanza (serve AC)
- Inverno (15¬∞C esterno) + GPU = beneficio riscaldamento!

**Soluzioni:**
- ‚úÖ AC split system stanza server (‚Ç¨500-1,000 install)
- ‚úÖ Ventilazione forzata (exhaust fan finestra)
- ‚úÖ Posiziona vicino a finestra (airflow naturale inverno)
- ‚ö†Ô∏è Monitora temperatura 24/7 (smart thermostat)

---

### Network & Latency

**LLM Inference Network Requirements:**

| Scenario | Bandwidth | Latency Target | Setup |
|----------|-----------|----------------|-------|
| **Local solo** | N/A | <5ms | Direct access |
| **LAN team** | 100 Mbps | <10ms | Gigabit switch ‚úÖ |
| **Multi-location** | 1 Gbps | <50ms | Firewall config + VPN |

**Setup raccomandato:**
```
[GPU Server] ‚Üí Gigabit Switch ‚Üí Firewall ‚Üí Internet
              ‚Üì
        [PC Staff LAN] ‚Üí FastAPI endpoint http://gpu-server:8000
```

**Configurazione firewall:**
- Port 8000 (FastAPI) aperto solo LAN interna
- NO esposizione internet pubblica (rischio security)
- Opzionale: Tailscale/Wireguard VPN per accesso remoto sicuro

---

### Power Management & UPS

**Rischi elettrici:**
- ‚ö° Spike/surge = GPU morte (‚Ç¨800-2,000 persi)
- üîå Blackout improvviso = corrupted VRAM/filesystem
- ‚ö†Ô∏è Voltage sag = reboot random

**Protezione necessaria:**

| Dispositivo | Costo | Beneficio |
|-------------|-------|-----------|
| **UPS 1000VA** | ‚Ç¨150-250 | Shutdown graceful 10-15min |
| **UPS 1500VA** | ‚Ç¨250-400 | 20-30min runtime |
| **Surge protector** | ‚Ç¨30-60 | Anti-spike (minimo) |

**RACCOMANDAZIONE:**
- ‚úÖ UPS 1000VA minimo (APC Back-UPS 1000)
- ‚úÖ Pure sine wave (GPU sensitive)
- ‚úÖ Software monitoring (apcupsd o NUT)
- ‚úÖ Auto-shutdown script quando battery <20%

---

### Backup & Disaster Recovery

**Cosa fare se GPU muore?**

**PIANO A - Spare GPU:**
- Tieni RTX 3060 12GB (‚Ç¨300 usata) come backup
- Swap in <30min, servizio ripristinato degraded
- Costo: ‚Ç¨300 assicurazione

**PIANO B - Cloud Failover:**
- Script auto-deploy su RunPod quando GPU locale down
- Health check ogni 5min ‚Üí failure ‚Üí deploy cloud
- Costo: ‚Ç¨0 standby + ‚Ç¨0.30/hr quando attivo

**PIANO C - Dual-GPU Redundancy:**
- 2x RTX 3090 in active-passive
- Primary muore ‚Üí secondary takes over
- Costo: +‚Ç¨900 CAPEX

**RACCOMANDAZIONE per albergo:**
- ‚úÖ Piano B (cloud failover) = best cost/benefit
- ‚úÖ + UPS per evitare failure da blackout
- ‚úÖ Monitoring con uptime alerts (Uptime Robot free tier)

---

### Manutenzione Schedulata

**Ogni 3 mesi:**
- Pulizia fisica (compressed air polvere)
- Verifica temperature (logs)
- Check fan noise (bearing usurato?)

**Ogni 6 mesi:**
- Repaste termica GPU (se temperature salite >5¬∞C)
- Verifica PSU fan (no polvere accumulo)
- Backup completo sistema

**Ogni anno:**
- Benchmark performance (vs baseline, degrado?)
- Valuta upgrade (nuove GPU NVIDIA release?)

**Costo manutenzione annuale:**
- DIY: ‚Ç¨50-100 (thermal paste, compressed air)
- Tecnico esterno: ‚Ç¨150-300 (1-2 interventi/anno)

---

### Security & Access Control

**Rischi:**
- üîì Accesso non autorizzato ‚Üí data leak
- üîì Abuse risorse (crypto mining da malware)
- üîì Model theft (IP proprietario)

**Hardening:**
- ‚úÖ Firewall: Solo LAN interna, NO internet pubblico
- ‚úÖ Authentication: API key su endpoint FastAPI
- ‚úÖ SSH: Key-based only, no password
- ‚úÖ User access: Sudo limitato, log audit
- ‚úÖ Monitoring: fail2ban, intrusion detection

---

## 11. RACCOMANDAZIONE FINALE

### TL;DR - Cosa Comprare OGGI per Albergo

**SCENARIO OTTIMALE:**

```
HARDWARE:
  GPU:     RTX 3090 24GB (usata eBay ~‚Ç¨850)
  CPU:     AMD Ryzen 7 5800X (‚Ç¨280)
  MOBO:    MSI B550 Gaming Plus (‚Ç¨130)
  RAM:     64GB DDR4-3600 (‚Ç¨200)
  SSD:     2TB NVMe Gen4 (‚Ç¨170)
  PSU:     Corsair RM850x 80+ Gold (‚Ç¨130)
  CASE:    Fractal Meshify 2 (‚Ç¨110)
  COOL:    Be Quiet Dark Rock Pro 4 (‚Ç¨80)
  UPS:     APC Back-UPS 1000VA (‚Ç¨200)
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  TOTALE:  ‚Ç¨2,150

SOFTWARE (FREE):
  OS:      Ubuntu 22.04 LTS
  Runtime: Ollama + FastAPI
  Models:  Qwen3-4B-Instruct Q4_K_M

CLOUD BACKUP:
  Provider: RunPod Community Cloud
  Usage:    DR failover + spike testing
  Costo:    ~‚Ç¨30-50/mese (uso occasionale)
```

**PERFORMANCE ATTESA:**
- Qwen3-4B: 80-90 token/s (latency ~12ms)
- Qwen3-7B: 55-65 token/s (latency ~18ms)
- Llama3-13B: 30-40 token/s (latency ~30ms)
- Mixtral-8x7B: 20-25 token/s (latency ~50ms)

**TCO 3 Anni:**
- CAPEX: ‚Ç¨2,150
- OPEX: ‚Ç¨2,700 (‚Ç¨900/anno elettricit√† + manutenzione)
- **TOTALE: ‚Ç¨4,850** = **‚Ç¨135/mese**

**Cloud Equivalente 3 Anni:**
- 10hr/giorno @‚Ç¨0.30/hr = ‚Ç¨986/anno
- **TOTALE: ‚Ç¨2,958** per 3 anni

**BREAK-EVEN: 27 mesi** (2.25 anni)

---

### Percorso Implementazione

**FASE 1: Proof of Concept (Mese 1-2) - ‚Ç¨250**
```
1. Test su Jetson Orin Nano Super (‚Ç¨249)
   ‚Üí Verifica workflow, sviluppo applicazione
   ‚Üí Zero rischio, vendibile dopo se non serve

2. O in alternativa: Cloud RunPod (‚Ç¨60-100 totale)
   ‚Üí 20-30 ore testing totali
   ‚Üí Valida architettura prima di HW commitment
```

**FASE 2: Production Build (Mese 3) - ‚Ç¨2,150**
```
1. Acquista componenti (priorit√† eBay per GPU)
2. Build sistema + Ubuntu install
3. Setup Ollama + FastAPI + monitoring
4. Migra da PoC a production
```

**FASE 3: Ottimizzazione (Mese 4-6) - ‚Ç¨300**
```
1. Fine-tuning modello su dati albergo (optional)
2. Setup cloud failover automation
3. Monitoring & alerting production-grade
4. Staff training uso sistema
```

**FASE 4: Scaling (Anno 2+) - ‚Ç¨900-4,000**
```
OPZIONE A: Aggiungi 2¬∞ GPU
  ‚Üí +RTX 3090 usata (‚Ç¨900)
  ‚Üí 48GB totale ‚Üí modelli 30B-70B

OPZIONE B: Upgrade GPU nuova gen
  ‚Üí Vendi 3090 (‚Ç¨600-700)
  ‚Üí Compra RTX 5080/5090 (‚Ç¨1,500-2,000)
  ‚Üí +30-50% performance

OPZIONE C: Status quo
  ‚Üí RTX 3090 ancora performante
  ‚Üí Reinvesti in cloud credits spike
```

---

### Decision Tree

```
HAI BUDGET ‚Ç¨500-800?
‚îÇ
‚îú‚îÄ SI ‚Üí Proof of Concept
‚îÇ   ‚îú‚îÄ Jetson Orin Nano Super (‚Ç¨249)
‚îÇ   ‚îî‚îÄ O RTX 3060 12GB usata + upgrade PC esistente (‚Ç¨500)
‚îÇ
‚îî‚îÄ NO ‚Üí HAI BUDGET ‚Ç¨1,500-2,500?
    ‚îÇ
    ‚îú‚îÄ SI ‚Üí ‚≠ê RTX 3090 Build Completa (‚Ç¨2,150) [RACCOMANDATO]
    ‚îÇ
    ‚îî‚îÄ NO ‚Üí HAI BUDGET ‚Ç¨4,000+?
        ‚îÇ
        ‚îú‚îÄ SI ‚Üí RTX 4090 Build o Dual RTX 3090
        ‚îÇ
        ‚îî‚îÄ NO ‚Üí Usa Cloud 6-12 mesi mentre risparmi
                 ‚Üí RunPod/Vast.ai ‚Ç¨50-150/mese
                 ‚Üí Poi build HW quando budget ready
```

---

### RED FLAGS - Quando NON fare Self-Hosting

‚ùå **Evita self-hosting SE:**
- Budget <‚Ç¨1,500 disponibile (PoC Jetson ok, production NO)
- Nessuna competenza Linux/Docker (curva apprendimento ripida)
- Uso <3 ore/giorno (cloud pi√π economico)
- Nessuno staff tech-savvy per manutenzione
- Location senza AC e temperatura >28¬∞C estate
- Elettricit√† >‚Ç¨0.40/kWh (costo proibitivo)

‚úÖ **Fai self-hosting SE:**
- Budget ‚Ç¨2k+ disponibile
- Almeno 1 persona tech-savvy nello staff
- Uso >8 ore/giorno prevedibile
- Privacy/GDPR requirement stringenti
- Orizzonte >2 anni utilizzo
- Infrastruttura fisica adeguata (AC, UPS, network)

---

### Alternative se Budget Limitato

**NON HAI ‚Ç¨2k ora? Fai questo:**

1. **START con Cloud (3-6 mesi)**
   - RunPod Community: ‚Ç¨80-150/mese
   - Valida use case reale
   - Misura ore/giorno utilizzo effettivo
   - Risparmia CAPEX nel frattempo

2. **POC con Jetson (‚Ç¨249)**
   - Test workflow applicazione
   - Sviluppo software su HW reale
   - Decision point dopo 2 mesi
   - Se fallisce: perdita limitata ‚Ç¨249

3. **Upgrade PC Esistente (‚Ç¨500-800)**
   - RTX 3060 12GB usata (‚Ç¨350)
   - Upgrade PSU 750W (‚Ç¨100)
   - Upgrade RAM +16GB (‚Ç¨60)
   - **Solo se** PC ha slot PCIe x16 disponibile

4. **Finanziamento Hardware**
   - Amazon rate (12 mesi 0% interesse)
   - PayPal Pay in 3/4
   - Leasing aziendale (se albergo = business)
   - TCO spread su 12-24 mesi

---

## 12. CONCLUSIONI & NEXT STEPS

### Sintesi Ricerca

**Hardware AI fisico NVIDIA per LLM self-hosting nel 2026 √®:**
- ‚úÖ **Tecnicamente fattibile** con budget ‚Ç¨500-5,000
- ‚úÖ **Economicamente conveniente** per uso >8hr/giorno (break-even 8-24 mesi)
- ‚úÖ **Scalabile** da 4B a 70B modelli con scelte HW appropriate
- ‚ö†Ô∏è **Richiede competenza** Linux/Docker/networking
- ‚ö†Ô∏è **Impegno manutenzione** ongoing (pulizia, monitoring, updates)

**VRAM √® il fattore limitante #1:**
- 12GB = max 13B quantizzati
- 24GB = sweet spot 4B-30B + scaling futuro
- 48GB+ = 70B+ comfortable

**Best Value 2026: RTX 3090 24GB usata (~‚Ç¨850)**
- Miglior $/VRAM sul mercato
- Performance 80% di RTX 4090 a 40% del costo
- Mercato usato maturo (mining flood)
- Scaling path chiaro (dual-GPU o upgrade futuro)

---

### Raccomandazione Specifica Albergo

**PER QWEN3-4B Q4_K_M:**

**OPZIONE RACCOMANDATA:**
```
RTX 3090 24GB Build Completa = ‚Ç¨2,150
‚îú‚îÄ Headroom VRAM ‚Üí scaling 7B, 13B, 30B futuro
‚îú‚îÄ Performance 80 token/s ‚Üí latency <15ms
‚îú‚îÄ Break-even 24 mesi @ 10hr/giorno
‚îî‚îÄ TCO 3yr = ‚Ç¨4,850 vs ‚Ç¨2,958 cloud
    (Cloud pi√π economico primi 2 anni, poi self-host vince)
```

**SE budget limitato oggi:**
```
Jetson Orin Nano Super = ‚Ç¨249 (PoC)
‚Üí Valida workflow 2-3 mesi
‚Üí Risparmia per RTX 3090 build nel frattempo
‚Üí Oppure continua cloud se use case fallisce
```

**SE serve scaling aggressivo futuro:**
```
RTX 4090 Build = ‚Ç¨4,000
‚Üí +40% performance vs 3090
‚Üí Architettura newer (supporto longer-term)
‚Üí Break-even 36 mesi (3 anni)
```

---

### Next Steps Operativi

**SETTIMANA 1-2: Decision & Budget**
- [ ] Conferma budget disponibile (‚Ç¨250 / ‚Ç¨2k / ‚Ç¨4k?)
- [ ] Valuta infrastruttura fisica albergo (spazio, AC, network)
- [ ] Identifica staff tech-savvy per manutenzione
- [ ] Decision: PoC Jetson vs Cloud vs Direct build

**SETTIMANA 3-4: Acquisizione (se build)**
- [ ] Ricerca GPU usata (eBay, Subito, forum)
- [ ] Compra componenti (priorit√† PSU + case per shipping time)
- [ ] Setup temporaneo cloud RunPod (backup durante build)

**MESE 2: Build & Setup**
- [ ] Assemblaggio fisico sistema
- [ ] Ubuntu 22.04 LTS install + CUDA drivers
- [ ] Ollama install + Qwen3-4B model download
- [ ] FastAPI endpoint + basic authentication
- [ ] Network integration (LAN interna albergo)

**MESE 3: Testing & Validation**
- [ ] Load testing (concurrent users)
- [ ] Latency benchmarking (<20ms target)
- [ ] Failure testing (GPU crash, power loss)
- [ ] Cloud failover automation setup
- [ ] Staff training uso API

**MESE 4+: Production & Monitoring**
- [ ] Monitoring 24/7 (Prometheus + Grafana)
- [ ] Alerting setup (email/SMS se GPU down)
- [ ] Manutenzione mensile scheduling
- [ ] Cost tracking vs cloud baseline

---

### Risorse & Fonti Utili

**Community & Support:**
- r/LocalLLaMA (Reddit) - Self-hosting community
- NVIDIA Developer Forums - Technical issues
- Ollama Discord - Software support
- HuggingFace Forums - Model optimization

**Tools & Software:**
- [Ollama](https://ollama.ai/) - Easiest LLM runtime
- [vLLM](https://github.com/vllm-project/vllm) - Production inference
- [FastAPI](https://fastapi.tiangolo.com/) - API framework
- [GPT-Z](https://www.techpowerup.com/gpuz/) - GPU monitoring

**Benchmarks & Comparisons:**
- [Hardware Corner LLM Database](https://www.hardware-corner.net/llm-database/)
- [Best Value GPU Tracker](https://bestvaluegpu.com/)
- [GPU Pricing Comparison](https://getdeploying.com/gpus)

---

### Domande Aperte per Follow-Up

1. **Infrastruttura fisica albergo:**
   - Disponibilit√† server room con AC?
   - Capacit√† elettrica dedicata?
   - Distanza da reception (rumore)?

2. **Use case specifico:**
   - Numero utenti concurrent previsti?
   - Ore/giorno utilizzo stimato?
   - Modelli oltre Qwen3-4B previsti?

3. **Competenze tecniche:**
   - Chi gestir√† manutenzione?
   - Esperienza Linux/Docker nello staff?
   - Budget formazione se necessario?

4. **Timeline:**
   - Urgenza deployment (1 mese? 6 mesi?)
   - Possibilit√† start con cloud mentre prepari HW?

---

## FONTI COMPLETE

### NVIDIA Jetson
1. [Jetson AGX Orin Official](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)
2. [Buy Jetson Products](https://developer.nvidia.com/buy-jetson)
3. [Running LLMs with TensorRT-LLM on Jetson](https://www.hackster.io/shahizat/running-llms-with-tensorrt-llm-on-nvidia-jetson-agx-orin-34372f)
4. [Getting Started with Edge AI on Jetson](https://developer.nvidia.com/blog/getting-started-with-edge-ai-on-nvidia-jetson-llms-vlms-and-foundation-models-for-robotics/)

### RTX Workstation GPUs
5. [RTX 6000 Ada vs 4090 for AI](https://www.bestgpusforai.com/gpu-comparison/6000-ada-vs-4090)
6. [RTX 4090 vs 6000 Ada for Self-Hosted vLLM](https://www.jolomo.io/rtx-4090-vs-rtx-6000-ada-for-self-hosted-vllm-interfaces/)
7. [Best NVIDIA GPUs for LLM Inference 2025](https://www.databasemart.com/blog/best-nvidia-gpus-for-llm-inference-2025)
8. [RTX 3090 vs 4080 Comparison](https://www.bestgpusforai.com/gpu-comparison/3090-vs-4080)
9. [Best GPU for Local LLM 2026](https://nutstudio.imyfone.com/llm-tips/best-gpu-for-local-llm/)

### DGX Systems
10. [NVIDIA DGX H100 Deep Learning Console](https://viperatech.com/product/nvidia-dgx-h100-deep-learning-console-640gb-sxm5)
11. [DGX H100 Price 2025 Cost & Specs](https://cyfuture.cloud/kb/gpu/nvidia-dgx-h100-price-2025-cost-specs-and-market-insights)
12. [H100 GPU Cost 2025 Buy vs Rent](https://www.gmicloud.ai/blog/how-much-does-the-nvidia-h100-gpu-cost-in-2025-buy-vs-rent-analysis)
13. [H100 Price Guide 2026](https://docs.jarvislabs.ai/blog/h100-price)

### Server Build Guides
14. [Build Your Own AI PC 2026](https://techpurk.com/build-ai-pc-specs-2026-local-llms/)
15. [LLM Server Hardware Guide 2026](https://novoserve.com/blog/how-to-run-an-llm-on-a-server-your-2026-llm-server-hardware-guide)
16. [Best AMD Motherboards Dual GPU 2026](https://www.propelrc.com/best-amd-motherboards-for-dual-gpu-llm-builds/)
17. [LLM Hardware Guide GPU RAM Storage](https://www.localai.computer/learn/llm-hardware-guide)

### Qwen Requirements
18. [GPU System Requirements Qwen Models](https://apxml.com/posts/gpu-system-requirements-qwen-models)
19. [Qwen3 Hardware Requirements Part II](https://dev.to/ai4b/comprehensive-hardware-requirements-report-for-qwen3-part-ii-4i5l)
20. [Qwen3 LLM Hardware Requirements](https://www.hardware-corner.net/guides/qwen3-hardware-requirements/)

### Power & Cooling
21. [RTX 4090 Power Consumption Guide](https://www.ecoenergygeek.com/rtx-4090-power-consumption/)
22. [GPU Server Thermal Management](https://www.electronics-cooling.com/2017/02/thermal-management-gpu-enabled-servers-data-centers/)
23. [Costs of Deploying AI Energy Cooling](https://www.exxactcorp.com/blog/hpc/the-costs-of-deploying-ai-energy-cooling-management)

### Cost Analysis
24. [Cost Analysis LLMs Cloud vs Self-Hosted](https://medium.com/artefact-engineering-and-data-science/llms-deployment-a-practical-cost-analysis-e0c1b8eb08ca)
25. [Cloud vs On-Prem LLMs Long-Term Cost](https://latitude-blog.ghost.io/blog/cloud-vs-on-prem-llms-long-term-cost-analysis/)
26. [Cost-Benefit Analysis On-Premise LLM (arXiv)](https://arxiv.org/html/2509.18101v1)
27. [LLM Total Cost of Ownership 2025](https://www.ptolemay.com/post/llm-total-cost-of-ownership)

### Cloud Pricing
28. [GPU Cloud Pricing Comparison 2026](https://getdeploying.com/gpus)
29. [RunPod vs Vast.ai GPU Pricing](https://computeprices.com/compare/runpod-vs-vast)
30. [Top Cloud GPU Providers 2026](https://www.runpod.io/articles/guides/top-cloud-gpu-providers)
31. [7 Cheapest Cloud GPU Providers](https://northflank.com/blog/cheapest-cloud-gpu-providers)

### Used GPU Market
32. [RTX 3090 Price Tracker January 2026](https://bestvaluegpu.com/history/new-and-used-rtx-3090-price-history-and-specs/)
33. [eBay NVIDIA GeForce RTX 3090 24GB](https://www.ebay.com/b/NVIDIA-GeForce-RTX-3090-24GB-GDDR6-Graphics-Cards/27386/bn_7117810176)
34. [RTX 4090 Price Tracker January 2026](https://bestvaluegpu.com/history/new-and-used-rtx-4090-price-history-and-specs/)

### Budget Options
35. [Best Budget GPUs for AI 2026](https://www.fluence.network/blog/best-budget-gpus/)
36. [Top 7 Best Budget GPUs for AI 2026](https://techtactician.com/best-budget-gpus-for-local-ai-workflows/)

---

**Fine Ricerca - 11 Gennaio 2026**

*Cervella Researcher - CervellaSwarm*
