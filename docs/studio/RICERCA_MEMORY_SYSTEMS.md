# RICERCA: Agent Memory Systems 2025-2026

> **Ricerca:** Cugino #1 (cervella-researcher)
> **Data:** 1 Gennaio 2026
> **Contesto:** PoC Cugini - Ricerca Parallela

---

## EXECUTIVE SUMMARY

La memoria Ã¨ LA capability fondamentale degli agenti AI moderni. Il 2025-2026 ha portato:
1. **Context window grandi â‰  buona memoria** - Servono architetture esplicite
2. **RAG da solo non basta** - Necessari hybrid approaches (graph + vector + semantic)
3. **Strategic forgetting = feature** - Non tutto va ricordato
4. **GraphRAG = standard emergente** - Knowledge graphs coordinano multi-agent
5. **Memory corruption = nuovo problema critico** - Serve validazione continua

---

## 1. STATE OF THE ART

### Il Problema Fondamentale
Gli LLM non hanno memoria nativa. Ogni conversazione parte da zero. Le soluzioni 2025-2026:

- **Context Window** - Claude: 200K tokens, GPT-4: 128K, Gemini: 1M+
- **Ma**: PiÃ¹ lungo â‰  migliore. Needle-in-haystack test mostra degradazione
- **Soluzione**: Architetture esplicite di memoria

### Trend Dominante: Memoria Multi-Livello
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WORKING MEMORY (Immediata)             â”‚
â”‚  - Context window attivo                â”‚
â”‚  - ~10K tokens efficaci                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHORT-TERM MEMORY (Sessione)           â”‚
â”‚  - Riassunti conversazione              â”‚
â”‚  - Key-value cache                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LONG-TERM MEMORY (Persistente)         â”‚
â”‚  - Vector DB (Pinecone, Chroma)         â”‚
â”‚  - Knowledge Graph                      â”‚
â”‚  - File system                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. PATTERN PRINCIPALI

### Pattern 1: MemGPT Virtual Memory
- **Concept**: Tratta LLM come CPU con memoria virtuale
- **Architettura**: Core Memory (RAM) + Recall Memory (Cache) + Archive (Disk)
- **Pro**: Gestione automatica paging
- **Contro**: Overhead computazionale

### Pattern 2: RAG Evolution (2025)
- **Vanilla RAG** â†’ problemi di retrieval accuracy
- **Agentic RAG** â†’ agent decide QUANDO e COSA recuperare
- **GraphRAG** â†’ knowledge graph + vector search ibrido
- **Best practice**: Query rewriting + reranking + hybrid search

### Pattern 3: Context Engineering
4 strategie fondamentali:
1. **Write Context** - Scrivi riassunti strutturati
2. **Select Context** - RAG per scegliere cosa includere
3. **Compress Context** - Summarization aggressiva
4. **Isolate Context** - Separazione per dominio

### Pattern 4: Shared Memory Multi-Agent
- **Blackboard Pattern** - Memoria condivisa centrale
- **Stigmergy** - Comunicazione indiretta via ambiente
- **Message Passing** - Queue per comunicazione
- **Per CervellaSwarm**: ROADMAP.md = stigmergy!

---

## 3. BEST PRACTICES

### Do's
- âœ… Struttura esplicita memoria (non solo context dump)
- âœ… Separazione per tipo (facts vs procedures vs episodes)
- âœ… Validazione periodica (memory puÃ² corrompersi)
- âœ… Forgetting strategico (non tutto va ricordato)
- âœ… Human-readable format (debug facile)

### Don'ts
- âŒ Affidarsi solo a context window grande
- âŒ Memorizzare tutto senza filtro
- âŒ Ignorare memory drift over time
- âŒ Mixing concerns in single memory store

---

## 4. LIMITI E SFIDE

### Problemi Non Risolti

| Problema | Descrizione | Mitigazione |
|----------|-------------|-------------|
| **Memory Corruption** | Info false persistono | Validazione periodica |
| **Context Drift** | Memoria diverge da realtÃ  | Refresh da source of truth |
| **Forgetting** | Quando dimenticare? | TTL + relevance scoring |
| **Coordination** | Multi-agent memory sync | Leader election + consensus |
| **Cost** | PiÃ¹ memoria = piÃ¹ token | Compression + caching |

---

## 5. APPLICABILITA CERVELLASWARM

### Architettura Raccomandata

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CERVELLASWARM MEMORY                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  LIVELLO 1: FILE-BASED (Attuale) âœ…                            â”‚
â”‚  â”œâ”€â”€ ROADMAP_SACRA.md (strategia)                              â”‚
â”‚  â”œâ”€â”€ PROMPT_RIPRESA.md (contesto sessione)                     â”‚
â”‚  â”œâ”€â”€ NORD.md (direzione)                                       â”‚
â”‚  â””â”€â”€ swarm_memory.db (eventi + lezioni)                        â”‚
â”‚                                                                 â”‚
â”‚  LIVELLO 2: DATABASE (Attuale) âœ…                              â”‚
â”‚  â”œâ”€â”€ swarm_events (log task)                                   â”‚
â”‚  â”œâ”€â”€ lessons_learned (knowledge base)                          â”‚
â”‚  â””â”€â”€ error_patterns (pattern detection)                        â”‚
â”‚                                                                 â”‚
â”‚  LIVELLO 3: FUTURO (Da Valutare)                               â”‚
â”‚  â”œâ”€â”€ Vector DB per semantic search                             â”‚
â”‚  â”œâ”€â”€ Knowledge Graph per relazioni                             â”‚
â”‚  â””â”€â”€ Real-time sync per multi-agent                            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Raccomandazioni Immediate

1. **Continuare con file-based** - Funziona, Ã¨ debuggabile, human-readable
2. **Potenziare SQLite** - GiÃ  in uso, aggiungere indici e views
3. **Stigmergy via ROADMAP** - Pattern validato per coordinamento
4. **Validazione settimanale** - Review memoria per evitare drift

### Raccomandazioni Future

1. **Vector search** - Per lezioni simili (semantic similarity)
2. **Knowledge Graph** - Per relazioni tra concetti (GraphRAG)
3. **Memory compression** - Per sessioni lunghe

---

## FONTI

1. MemGPT Paper (Berkeley, 2023-2024)
2. LangChain Memory Documentation (2025)
3. Anthropic Context Engineering Guide
4. Microsoft AutoGen Memory Patterns
5. OpenAI Best Practices for LLM Memory
6. Google ADK Agent Memory Architecture
7. Pinecone RAG Best Practices 2025
8. Neo4j GraphRAG Documentation

---

*"La memoria Ã¨ ciÃ² che trasforma un LLM in un agente."* ğŸ§ 

*Ricerca completata da Cugino #1 - PoC Parallelizzazione* ğŸ
